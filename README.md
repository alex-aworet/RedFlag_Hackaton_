# ğŸ™ï¸ RedFlag â€“ DÃ©tection de ToxicitÃ© en Jeu VidÃ©o

## ğŸ“Œ Description
**Red Flag** est un systÃ¨me de **dÃ©tection en temps rÃ©el de toxicitÃ© vocale** dans les jeux en ligne.  
Le projet sâ€™appuie sur la **transcription de la voix en texte (speech-to-text)**, puis sur un module de classification qui identifie les propos offensants, injurieux ou menaÃ§ants.  

Lorsquâ€™un joueur adopte un comportement verbal toxique :  
- Un **score dâ€™intensitÃ©** est calculÃ©.  
- Si ce score dÃ©passe un certain seuil, un **avertissement** est gÃ©nÃ©rÃ©.  
- AprÃ¨s plusieurs avertissements, une **sanction automatique** est applliquÃ©e : Fermeture automatique du jeu.  

Lâ€™objectif est de **lutter contre la toxicitÃ© en ligne** et de protÃ©ger les joueurs dâ€™expÃ©riences nocives.

Nos tests se basaient principalement l'usage de l'Ã©mulateur ''ppsspp'' dont nous avons adaptÃ© les paramÃ¨tres rÃ©seaux afin de correspondre aux exigences de notre algorithme de surveillance.

---

## âš™ï¸ FonctionnalitÃ©s
- ğŸ¤ Capture et transcription en direct de la voix des joueurs.  
- ğŸ¤– Classification IA des propos avec un modÃ¨le entraÃ®nÃ© sur +25 000 insultes et comportements toxiques.  
- ğŸš¨ SystÃ¨me de sanctions progressives (avertissements â†’ sanctions).  
- ğŸ“¡ DÃ©clenchement automatique via la surveillance du trafic rÃ©seau (`adhoc_monitor.py`).  
- ğŸ“ Sauvegarde des transcriptions dans un fichier (`transcription.txt`).  

---

## ğŸ“‚ Structure du projet
```
â”œâ”€â”€ adhoc_monitor.py         # Sniffer rÃ©seau qui dÃ©clenche la dÃ©tection --> code Ã  lancer
â”œâ”€â”€ live_voice_detection.py  # DÃ©tection et transcription vocale en direct
â”œâ”€â”€ main.py                  # Gestion des trigger --> ne pas jouer directement
â”œâ”€â”€ mistral.py               # ModÃ¨le de classification (LLM Mistral)
â”œâ”€â”€ run.py                   # Orchestration du pipeline : classification + execution de la sanction
â”œâ”€â”€ sanctions.py             # Gestion des avertissements et sanctions
â”œâ”€â”€ utils.py                 # Fonctions utilitaires requises pour la surveillance
â”œâ”€â”€ utils_classification.py  # Outils pour la classification
```

---

## ğŸš€ Installation

### 1. PrÃ©requis
- Python 3.9+  
- `pip` ou `conda`  
- Microphone et accÃ¨s rÃ©seau  

### 2. Cloner le projet
```bash
git clone https://github.com/alex-aworet/RedFlag_Hackaton_.git
cd RedFlag_Hackaton_
```

### 3. CrÃ©er un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 4. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Utilisation

### Lancer la dÃ©tection de traffic en direct
```bash
python adhoc_monitor.py
```

Ce code permettra que lorsque qu'un Ã©change de donnÃ©es en ligne est repÃ©rÃ©, les algorithmes de speech to text, de classification de parole ainsi que sanction seront exÃ©cutÃ©s en parallÃ¨le.

---

## âš ï¸ Cas d'usage

Dans notre expÃ©rimentation, nous avons utilisÃ© un Ã©mulateur de psp pour les simulations. Il suffit de lancer `adhoc_monitor.py` qui dÃ©clenchera les fonctions perception de voix, classification et sanction en parrallÃ¨le dÃ¨s le moment oÃ¹ une partie en ligne est dÃ©tectÃ©e.

Dans le cadre de la prise de dÃ©cision, nous avons entrainÃ© un modÃ¨le small de mistral ai sur les datasets suivant :

- [Jigsaw Toxic Comment Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)  
- [Hatebase](https://www.hatebase.org/)  
- Selected in-game chat logs for slang and context 

et avons dÃ©cidÃ© de labeliser comme suit :

| Score | Category |
|-------|----------|
| 1â€“2   | Non-toxic |
| 3â€“4   | Mild insult |
| 5â€“7   | Strong insult/obscene |
| 8â€“10  | Severe hate/threat |

---

## ğŸ“œ Licence
Ce projet est sous licence **MIT** â€“ utilisation libre et open-source.  
